{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Downloading gpt2 model to /aitextgen.\n",
      "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "470052b013f6463d94b4fa12f06e3cc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "73c743f3ad914610994b8e6de0de6e1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen(to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========1=========\n",
      "Fisherman's Cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "Fisherman's Cove in the heart of the Pacific Ocean has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the heart of the Pacific Ocean on the Hawaiian Islands. The cove has the potential to provide the perfect home for a family of four or more.\n",
      "\n",
      "The cove is located in the\n",
      "========2=========\n",
      "\n",
      "How to use it:\n",
      "\n",
      "When you are prompted to enter a password, click \"OK\". To confirm, click \"OK\" in the top-right corner of the screen.\n",
      "\n",
      "When prompted again, click \"Confirm\".\n",
      "\n",
      "When prompted again, click \"Ok\".\n",
      "\n",
      "Once you have confirmed that you are entering a password, the system will ask you to provide your password again. This time, the user will be given the option to enter the password again.\n",
      "==========\n",
      "\n",
      "The US has pledged to increase its military spending to $1.5 trillion by 2020. The goal is to have each of the US' three largest military powers spend at least $100 billion on defense in 2020.\n",
      "\n",
      "However, there are many other measures to support the US in future – from building up its military to investing in new technologies to develop a more sophisticated cyber defense.\n",
      "\n",
      "\"We are still in the process of building up our military capabilities globally,\" US Defence Secretary Ash\n",
      "==========\n",
      "\n",
      "In my opinion, the most important thing to remember about this event is that it made me the most comfortable person to attend the event. I don't think I'm going to be getting any attention for it, but because of the sheer numbers, it's something I felt I needed to do.\n",
      "\n",
      "I got in a car that used to drive me about 50 miles to go to a party. It was in the middle of nowhere, but it was so easy and it had a great\n",
      "========3=========\n",
      "\u001B[1mI believe in unicorns because\u001B[0m they're not just for girls but are for women,\" Ms. Breen says, adding that even if a woman is the head of a company, she needs to be able to work with them and work with her family.\n",
      "\n",
      "\"This business is not about getting paid, it is about how you do business, about how you do business with your business partners,\" she says. \"You have to be able to work with your business partners and with your clientele\n",
      "==========\n",
      "\u001B[1mI believe in unicorns because\u001B[0m they make the world a better place if they can turn that into a healthy and prosperous one.\n",
      "\n",
      "A lot of the people who have been in it for many years now are very well-meaning people, and I think it's time that we recognize the things that people have done in the past.\n",
      "\n",
      "But at the same time, we also need to recognize that no matter what, there is still a way to go if you want to be a good person\n",
      "==========\n",
      "\u001B[1mI believe in unicorns because\u001B[0m they are real beings and not just some imaginary creature. They are not real creatures. They are human beings and not just some imaginary creature. And this is why they have been known to be very good friends.\"\n",
      "\n",
      "\"But the thing is, it is just me.\n",
      "\n",
      "\"I'm just you and my friend.\n",
      "\n",
      "\"I said that you were right. It is not like I am, it is about me.\n",
      "\n",
      "\"And if I\n"
     ]
    }
   ],
   "source": [
    "print(\"========1=========\")\n",
    "ai.generate()\n",
    "\n",
    "print(\"========2=========\")\n",
    "ai.generate(n=3, max_length=100)\n",
    "\n",
    "print(\"========3=========\")\n",
    "ai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)\n",
    "# ai.generate_to_file(n=10, prompt=\"I believe in unicorns because\", max_length=100, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mI believe in unicorns because\u001B[0m unicorns are a spiritual gift. They are the manifestation of God's power. No one else can do this. We believe in unicorns because unicorns are the manifestation of God's power.\n",
      "\n",
      "The first three letters of the alphabet are a symbol of God's gift. For instance, the letter \"A\" is the symbol of the power of the letter \"R.\" The letter \"C\" is the symbol of the power of the letter \"A.\"\n",
      "\n",
      "There are three letters of the alphabet that are used to represent God. The letter \"C\" is also a symbol of the power of the letter \"I.\" The letter \"R\" is also the symbol of the power of the letter \"A.\"\n",
      "\n",
      "The letter \"I\" is a symbol of the power of the letter \"r.\" The letter \"K\" is the symbol of the power of the letter \"I.\" The letter \"A\" is the symbol of the power of the letter \"R.\" The letter \"C\" is the symbol of the power of the letter \"C.\"\n",
      "\n",
      "The letter \"A\" is the symbol of the power of the letter \"R.\" The letter \"R\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"K\" is the symbol of the power of the letter \"R.\" The letter \"A\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"C.\" The letter \"K\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"C.\" The letter \"R\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"R.\" The letter \"I\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"R.\" The letter \"B\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"R.\" The letter \"B\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"B\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"B\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"R.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"B\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"B\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"B\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"I\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"B\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"I\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"R\" is the symbol of the power of the letter \"I.\"\n",
      "\n",
      "The letter \"R\" is the symbol of the power of the letter \"I.\" The letter \"I\" is the symbol of the power\n"
     ]
    }
   ],
   "source": [
    "# 10,000 max length may produce ~1000 words but they veer widely off topic\n",
    "ai.generate(n=1, prompt=\"I believe in unicorns because\", max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mMary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\u001B[0m\n",
      "\n",
      "\"It was very difficult to write a memoir on my own,\" she says. \"I was a very quiet person, so I didn't want to write anything that people would consider to be a memoir. But I had this idea of writing a memoir and I was like, 'I can write a memoir.'\"\n",
      "\n",
      "When she was 16, she was a prisoner at the local prison, but she didn't know much about what she wanted to write. \"I was like, 'I can write a memoir.' I didn't know much about the country. I didn't know what I wanted to write about.\"\n",
      "\n",
      "She worked as a writer on her memoir for eight years before deciding to take the leap to writing for a magazine called The New York Times. \"I started writing fiction,\" she says.\n",
      "\n",
      "The new book, \"The Boy Who Lived,\" calls for a darker version of life, one where the people around you are your friends, your family and your community.\n",
      "\n",
      "\"This book is about love, and it is about who you are as a person, as a person, and as a person. And it is so very personal,\" she says.\n",
      "\n",
      "\"I always write this kind of non-fiction and it is so personal to the people I'm writing about.\"\n",
      "\n",
      "The memoir takes place in New York City, and the book has been translated into more than 100 languages. The book was published in the United States in 2004.\n"
     ]
    }
   ],
   "source": [
    "ai.generate(n=1, prompt=\"Mary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\", max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More repetitive, but safer, less creative output\n",
    "ai.generate(n=1, prompt=\"Mary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\", max_length=100, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune CPU model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "config = GPT2ConfigCPU()\n",
    "#config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=16, num_steps=5000)\n",
    "ai.train(data, batch_size=4, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CPU Shakespeare model\n",
    "ai = aitextgen(model=\"small_models/cpu_shakespeare/pytorch_model.bin\", config=\"small_models/cpu_shakespeare/config.json\",\n",
    "               vocab_file=\"small_models/cpu_shakespeare/aitextgen-vocab.json\", merges_file=\"small_models/cpu_shakespeare/aitextgen-merges.txt\")\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune GPU GP2 model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "# config = GPT2ConfigCPU()\n",
    "config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=16, num_steps=5000)\n",
    "ai.train(data, batch_size=4, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPU GPT2 Shakespeare model\n",
    "ai = aitextgen(model=\"large_models/gpu_gpt2_shakespeare/pytorch_model.bin\", config=\"large_models/gpu_gpt2_shakespeare/config.json\",\n",
    "               vocab_file=\"large_models/gpu_gpt2_shakespeare/aitextgen-vocab.json\", merges_file=\"large_models/gpu_gpt2_shakespeare/aitextgen-merges.txt\",\n",
    "               to_gpu=True)\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}