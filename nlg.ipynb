{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 124M Base Model\n",
    "Run the cell below to use the base GPT2 124M file to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Loading gpt2 model from /aitextgen.\n",
      "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========1=========\n",
      "\"You can't ask for more from the administration than you can give. And when you look at it, it's not about what's in your heart. It's about what you're willing to compromise with the president, and what you're willing to listen to the president.\" â€”Sen. Marco Rubio (R-FL)\n",
      "\n",
      "Sen. Marco Rubio, R-Florida, who has been under fire for his statements about the administration's policies in Syria, said Friday he's open to the president's proposed budget.\n",
      "\n",
      "\"I'm open to the president's proposal, which is a better alternative to the current administration's budget than what we've been negotiating with the administration,\" Rubio said on ABC's \"This Week.\" \"And I'm very willing to listen to the president's proposals.\"\n",
      "\n",
      "Rubio also criticized former President Barack Obama's decision to make the U.S. military stay open in Syria, saying that \"the world needs to look at this as an opportunity to do more than just pull out of a war that we're in. It's an opportunity to stand up to the president.\"\n",
      "\n",
      "\"I want to make clear what I think is important: I think we need to be working with the administration to find a better plan that\n",
      "========2=========\n",
      "I've long been interested in the history of the solar system. It was long ago that the solar system formed, but it was long ago that we discovered it. I've always assumed that this is a myth until I read some of the literature on the origins of the solar system.\n",
      "\n",
      "I've been searching for a book that could shed light on the history of the solar system, and I'm still searching for it. I've reached out to a handful of folks who've shared their\n",
      "==========\n",
      "\n",
      "The US Supreme Court's lower court upheld a lower court's decision to allow gay people to adopt, according to a report out today.\n",
      "\n",
      "In 2012, the high court said the same-sex couples who want to adopt two children should be allowed to adopt. The court ruled in U.S. v. Windsor that gay couples were not entitled to adopt.\n",
      "\n",
      "In the justices' ruling, the court said the right to adopt was \"consistent\" with the state of the\n",
      "==========\n",
      "\n",
      "By James R. Nieder\n",
      "\n",
      "December 2014\n",
      "\n",
      "In the wake of the Paris attacks, the U.S. government has been seeking to silence those who dare to speak out against terrorism. As such, the FBI has quietly detained a senior counterterrorism official in the U.S.\n",
      "\n",
      "The FBI is now conducting an unprecedented number of interviews of senior U.S. officials, including senior officials at the State Department, U.S. embassies, U.N. high\n",
      "========3=========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m it allows you to have a unique combination of flavors, textures and textures. I believe in the \"naturally\" produced \"white-knuckled\" chocolate that you get with a chocolate bar. The chocolate is pureed and then melted into the bars, then then pressed into a chocolate bar.\n",
      "\n",
      "I think chocolate bars are really a perfect substitute for plain chocolate and I believe that the \"natural\" flavor is what you get when you use chocolate in a bar\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m they're all people's favorite. They're the most popular. I'm not a fan of unicorns, but they're the best.\n",
      "\n",
      "The people who are the most popular are the people who are most passionate about the topic. I think of unicorns as people who love each other. I think of unicorns as people who really want to make their living doing what they do. I think of unicorns as people who are trying to make their way to\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m they are the only species that can live in nature and I believe that unicorns are the only creature in the universe that can go on life as a living being. So unicorns are the only species we can consider living in an infinite universe.\"\n",
      "\n",
      "\"The universe is infinite. So what you're saying is that we have to look at the universe at all. Or we can't look at it at all. The universe is infinite. And that's the key\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen(to_gpu=True)\n",
    "\n",
    "# Key parameters include n for number of samples, max_length to motivate longer/shorter text, temperature\n",
    "# (defaults to 1.2) for greater creativity (higher) or more conservative (but repetitive) output (lower),\n",
    "# prompt, which can be left as None.\n",
    "print(\"========1=========\")\n",
    "ai.generate()\n",
    "\n",
    "print(\"========2=========\")\n",
    "ai.generate(n=3, max_length=100)\n",
    "\n",
    "print(\"========3=========\")\n",
    "ai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Finetuning\n",
    "Run cell below to carry out CPU finetuning using the Shakespeare's plays dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Finetune CPU model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "config = GPT2ConfigCPU()\n",
    "#config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "ai.train(data, batch_size=16, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Finetuning\n",
    "Run cell below to carry out GPU finetuning using the Shakespeare's plays dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Finetune GPU GPT2 model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# Config for GPT2 GPU training model\n",
    "config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "ai.train(data, batch_size=16, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model\n",
    "Once training is complete, you should have the following 4 files that you can later use to load the model:\n",
    "1. Vocab file: In above examples, this will be ```aitextgen-vocab.json``` in root folder.\n",
    "2. Merges file: In above examples, this will be ```aitextgen-merges.txt``` in root folder.\n",
    "3. Config file: In above examples, this will be ```config.json``` in ```trained_model``` folder within the root folder.\n",
    "4. Model file: In above examples, this will be ```pytorch.bin``` in ```trained_model``` folder within the root folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model\n",
    "Run the cell below to load and use the CPU-trained GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Loading GPT-2 model from provided small_models/cpu_shakespeare/pytorch_model.bin.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "This's the good lord, and not;\n",
      "Your father of thee to the time:\n",
      "What, he am not a brother.\n",
      "\n",
      "KING RICHARD II:\n",
      "We do not, I know you,\n",
      "I am I am you, and my lord, I am a crown?\n",
      "\n",
      "CAMILLO\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "ai = aitextgen(model=\"small_models/cpu_shakespeare/pytorch_model.bin\", config=\"small_models/cpu_shakespeare/config.json\",\n",
    "               vocab_file=\"small_models/cpu_shakespeare/aitextgen-vocab.json\", merges_file=\"small_models/cpu_shakespeare/aitextgen-merges.txt\",\n",
    "               to_gpu=True)\n",
    "ai.generate(1, prompt=\"ROMEO:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
