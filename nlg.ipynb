{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to aitextgen to have it run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~~{.python}\n",
    "# aitextgen/aitextgen.py\n",
    "# Temp fix\n",
    "del train_params[\"show_progress_bar\"]\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(train_model)\n",
    "\n",
    "# transformers/tokenization_gpt2.py\n",
    "def convert_tokens_to_string(self, tokens):\n",
    "    \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "    # Remove instances of None in tokens\n",
    "    tokens = list(filter(lambda a: a is not None, tokens))\n",
    "    text = \"\".join(tokens)\n",
    "    text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
    "    return text\n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Loading gpt2 model from /aitextgen.\n",
      "INFO:aitextgen:Using the default GPT-2 Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\n",
    "ai = aitextgen(to_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========1=========\n",
      "\n",
      "The United States has been engaged in a war with Afghanistan since the end of the Cold War. But at least that's what the U.S. government believes.\n",
      "\n",
      "In the first decade of the 21st century, the U.S. has been involved in more wars than any other country in the world. This includes wars in Iraq and Afghanistan, the Middle East, the Balkans, and East Africa.\n",
      "\n",
      "But as the U.S. has been waging them, the only country in the world that has not been involved in one of them is Afghanistan. The Taliban is not the only force in the Middle East to be fighting the U.S.-led war on terrorism.\n",
      "\n",
      "The Taliban is also the only country in the world that has not been involved in a war with the U.S. or its allies.\n",
      "\n",
      "This is according to the U.S. official who heads the U.N. Security Council. He is the former head of the U.N. peacekeeping mission in Afghanistan.\n",
      "\n",
      "The U.S. official said that all of the Afghan government forces in Afghanistan are in place and that the Taliban has been fighting for several years.\n",
      "\n",
      "The U.S. government claims that the Taliban is on\n",
      "========2=========\n",
      "\n",
      "The first of these will be the first ever full-screen TV set. The second will be the next device to be made available: the HTC One M8, which also has a full-screen TV.\n",
      "\n",
      "This is all part of the HTC One M8's $7,999 price tag.\n",
      "\n",
      "The HTC One M8 is not the first device to be made available in the US. The HTC One M7 is made by HTC and is priced at $8,\n",
      "==========\n",
      "\n",
      "The Washington Post reported that the U.S. intelligence community had detected \"suspicious\" links between Russian intelligence officials and the Trump campaign during the 2016 presidential election.\n",
      "\n",
      "The Post later added that the U.S. government has since provided \"no new evidence to suggest that Russian officials have engaged in any coordination with Mr. Trump or his associates.\"\n",
      "\n",
      "The latest disclosures show that the intelligence community concluded that the Trump campaign had contacts with Russian intelligence in 2016, at least when it\n",
      "==========\n",
      "\n",
      "KARACHI: A small group of policemen are investigating a murder, the first case of its kind in the country, in a police station, in Karachi.\n",
      "\n",
      "The police in question, Kalyan Kalyan, are looking for the suspected killers of a policeman to be released.\n",
      "\n",
      "The police have been searching in the district for four of the policemen accused of carrying out the murder.\n",
      "\n",
      "The police station in the district has a few shops and the police have\n",
      "========3=========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m I love them. And unicorns are more than just a symbol. They're a symbol of love. And unicorns are a sign of peace.\n",
      "\n",
      "I believe in unicorns because I love them. And unicorns are more than just a symbol. They're a symbol of peace. I believe in unicorns because I love them. And unicorns are more than just a symbol. They're a sign of peace. I believe in unicorns because I love\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m they're the most beautiful thing on earth at the moment, and unicorns are only one of the many things that are possible in the world.\"\n",
      "\n",
      "But the group is not the only one. A group of young men believe that the world doesn't need any more unicorns. In fact, the group holds a massive rally this year dedicated to celebrating the first time a unicorn has appeared on Earth.\n",
      "\n",
      "\"I'm from a very small town in Pennsylvania,\n",
      "==========\n",
      "\u001b[1mI believe in unicorns because\u001b[0m of their power and their ability to bring all the people together.\" And it's not just the people who are going to benefit from these unicorns. They also bring out the best in them, and they're going to be part of the big picture for us.\n",
      "\n",
      "To understand what's happening in the world right now, let's talk about the Chinese economy. Last year, an astounding 8 percent of Chinese GDP was in China. In 2015, the Chinese economy\n"
     ]
    }
   ],
   "source": [
    "print(\"========1=========\")\n",
    "ai.generate()\n",
    "\n",
    "print(\"========2=========\")\n",
    "ai.generate(n=3, max_length=100)\n",
    "\n",
    "print(\"========3=========\")\n",
    "ai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)\n",
    "# ai.generate_to_file(n=10, prompt=\"I believe in unicorns because\", max_length=100, temperature=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mI believe in unicorns because\u001b[0m unicorns are the best way to show off your personality and your strengths. But I also believe in unicorns because I love them. I love them because they're always fun to play with and have fun with. I love them because they're the perfect way to show people a certain kind of person. I don't care if they're very attractive or not. I love them because they're the coolest thing in the world. They'll be just as cool as they are on stage. They're the best things to do for your family. They're the best things to do for your friends. They're the best things to do for you. And I think that's what it means to be able to have a unicorn for friends and family and be happy with them.\n",
      "\n",
      "What are your favorite items?\n",
      "\n",
      "I mean, it's pretty cool to have a little unicorn for my birthday present. But I think it's really cool to have a little unicorn for my graduation. And the most awesome thing I'll ever have is a unicorn for my birthday present. I'll never know!\n",
      "\n",
      "What are you most proud of?\n",
      "\n",
      "I'm pretty proud of my body. I'm pretty proud of my hair. I'm proud of my body. I'm proud of my personality. I'm proud of my intelligence. I'm proud of my strength. And I think I've had a lot of success with my body. I've done a lot of great things with my body. But I just know that all my body parts are special and I'm so proud of my body for being able to do that.\n",
      "\n",
      "What is your biggest challenge in life?\n",
      "\n",
      "I think that's a really big one. I think that's a really big one. I think that's another huge one. But I don't think it's the biggest challenge. Maybe it's the biggest challenge for me. But I think I have a lot of great things to be happy about.\n",
      "\n",
      "Did you ever get the urge to go outside?\n",
      "\n",
      "I never got the urge to go outside. I think that's a pretty good advice.\n",
      "\n",
      "What's your favorite thing about living in a city?\n",
      "\n",
      "Living in a city is pretty amazing. I think it's pretty awesome that you have a certain amount of free time. I think that's pretty awesome.\n",
      "\n",
      "Do you have any advice for anyone who wants to be a better person?\n",
      "\n",
      "I think you have to be the best at everything, but you have to be the best at everything. The last thing you want to do is become a celebrity. You know, you're the person who gets to go to the beach and go to the movies and get to do a concert, and you just have to create and have fun and put yourself in the spotlight. That's what I want to do.\n",
      "\n",
      "What are your favorite foods?\n",
      "\n",
      "I just can't seem to find that one.\n",
      "\n",
      "What are your favorite foods?\n",
      "\n",
      "What are your favorite foods? I'm not even sure. I have a lot of amazing recipes. I put together a bunch of crazy recipes. I have a lot of great recipes. I put together a bunch of crazy recipes.\n",
      "\n",
      "How did you get into fashion?\n",
      "\n",
      "I got into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. I was into fashion when I was 15 years old. And I was into fashion when I was 15 years old.\n",
      "\n",
      "What's your favorite way to express yourself?\n",
      "\n",
      "I always love to go out and have fun. I always love to go out and have fun. I think I always like to go out and have fun and have fun. I think I always like to go out and have fun. I think I always like to go out and have fun.\n",
      "\n",
      "What's your favorite way to express yourself?\n",
      "\n",
      "I don't think it's a secret anymore. I think there's a lot of people who don't think it's a secret anymore. I think there's a lot of people who don't think it's a secret anymore.\n",
      "\n",
      "What's your favorite way to express yourself?\n",
      "\n",
      "I like to go out and have fun and have fun. I think there's a lot of people who don't think it's a secret anymore.\n",
      "\n",
      "What's your favorite way to express yourself?\n",
      "\n",
      "I like to do things on my own time. I don't want to be on my own time. I want to do things on my own time.\n",
      "\n",
      "What's your favorite way to express yourself?\n",
      "\n",
      "I think I like to do things on my\n"
     ]
    }
   ],
   "source": [
    "# 10,000 max length may produce ~1000 words but they veer widely off topic\n",
    "ai.generate(n=1, prompt=\"I believe in unicorns because\", max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\u001b[0m She was afraid that no one would ever know how to love John, but she did not want anyone to know how to love John. John became more and more distant, afraid that John would not be with her with him, and at the same time, he was still John's friend.\n",
      "\n",
      "\n",
      "In his first year in prison, John had a good time in school. He was a good student, but he was also very much in love with his sister. He was a good boy, and his sister would often tell him that he should learn to love. He was not the type of boy who would go to school and be bullied, but he was a good boy, and he loved his sister. The only thing that made his sister feel jealous of his school was that she was his best friend. He would often tell her something about his sister that her sister would never see. He was afraid that he would not be able to love her because he was afraid that he would never feel the same with her. As he became more and more in his own way, he became more and more bitter to her, and would often tell her that he would never love her, because he was afraid that if he couldn't love her, he couldn't love John. He would often say to her: \"John, you may love me, but I cannot love you. I love you, but I am not your friend.\" He would sometimes get frustrated with her when she would say that he wanted to be with her, but he would always say to her: \"John, you will never love me again, but you will love John.\" In other words, he would never ever love her again.\n",
      "\n",
      "\n",
      "John was not a good boy. He had problems with his sister. He was depressed, and he had difficulties with his sister. He tried to stay away from her, but was afraid that if he was going to be with her, he would never be with her again. He was very sad and angry about his sister. He was scared about what he would do with her. He was a nice man, but he was also very unhappy. He hated his sister for being a girl, and he felt that he was only able to love John because he was so beautiful. He was very sad and angry about his sister for not being a girl.\n",
      "\n",
      "\n",
      "John was very angry about having to go to school with his friends. He was very angry and angry about his sister getting into trouble with the school. He was very upset about having to go to school with his friends. He felt that he was not the type to be in a relationship with his sister. He was very upset that his sister was going to be in prison for a long time. He was angry that he was going to go to school with his friends. He was angry that he was going to go to school with his sister. He felt that he was not a good boy. He was angry that he was going to be with his sister. He was very angry that he was going to go to school with his friends. He was angry that he was going to be with his sister. He was very angry that he was going to be with his sister. He made a mistake. He thought that if he was to be with his sister, he would never be with her again. He was very angry about the fact that he did not want to be with his sister. He was very angry that he did not want to be with his sister. He was very angry that he did not want to be with his sister. He felt that he was not a good boy. He was angry that he was going to be with his sister. He had problems with his sister. He was depressed, and he had problems with his sister. He was very sad and angry about his sister for not being a girl. He was very angry that he was going to be with his sister. He was very mad that he had to go to school with his friends. He was mad that he had to go to school with his friends. He was mad that he had to go to school with his friends. He felt that he was not the type to be in a relationship with his sister. He was very angry that he was going to be with his sister.\n",
      "\n",
      "\n",
      "He was not an easy boy. He was very angry when he was in jail. He was angry when he had to go to school with his friends. He was angry when he had to go to school with his friends. He was angry when he had to go to school with his friends. He felt that he was not the type to be in a relationship with his sister. He was angry when he had to go to school with his friends. He was angry when he had to go to school with his friends. He was angry when he had to go to school with his friends. He felt that he was not the type to be in a\n"
     ]
    }
   ],
   "source": [
    "ai.generate(n=1, prompt=\"Mary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\", max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\u001b[0m\n",
      "\n",
      "\"I was in prison for a long time, and I was a very good person,\" she said. \"I was very close to John, and I was very close to my family. I had a lot of friends, and I had a lot of friends who were very close to John. I was very close to my family. I was very close to my friends\n"
     ]
    }
   ],
   "source": [
    "# More repetitive, but safer, less creative output\n",
    "ai.generate(n=1, prompt=\"Mary loved John with all her heart, but the stresses of being in prison were causing an inevitable rift in their relationship.\", max_length=100, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n",
      "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425e20ea0a674ef990d94b23b67eeee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=40000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 40,000 sets of tokens from data/input.txt.\n",
      "WARNING:aitextgen:pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8720c882204469af515e3d00a2c289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ";\n",
      "And me to the this am I I\n",
      "\n",
      "\n",
      "I I the.\n",
      "QUEEN:\n",
      "\n",
      "\n",
      "\n",
      "I? you thee?\n",
      "\n",
      "I:\n",
      "The that\n",
      "\n",
      "\n",
      "But it.\n",
      "My be is the to my a'll you.\n",
      "\n",
      "That of.\n",
      "And to?\n",
      "\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "\n",
      "And the daughter:\n",
      "What, and the king,\n",
      "To is the cion;\n",
      "The one, I have not not I will.\n",
      "\n",
      "But then,\n",
      "And all a father!\n",
      "And a be good you.\n",
      "\n",
      "\n",
      "My that be not; I was's not will you's\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " me,\n",
      "His life, she was my lord! I have I be\n",
      "Now, the lord,\n",
      "I have not will to you? you have to our crown?\n",
      "\n",
      "Ay, I that will do have have not be you is,\n",
      "\n",
      "\n",
      "That not be his lord, I'll will it\n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ":\n",
      "And thou, thou, and your name,\n",
      "And and I were my lord.\n",
      "\n",
      "Why thou thou say you are this well,\n",
      "\n",
      "First Murderer:\n",
      "And and you now, thou, my lord,\n",
      "My king, I are a time.\n",
      "\n",
      "\n",
      "Provost:\n",
      "And make\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n",
      "INFO:lightning:Saving latest checkpoint..\n",
      "INFO:aitextgen:Saving trained model pytorch_model.bin to /trained_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      ";\n",
      "I have not no; or thou; and that will be an king:\n",
      "I have I'll not be a man?\n",
      "\n",
      "\n",
      "ANGELO:\n",
      "I have do to the man?\n",
      "\n",
      "\n",
      "JULIET:\n",
      "I'll I will I had now we do a ccee.\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "To give you, to not your lord;\n",
      "I have a good that's blood of a a\n",
      "Of a king and all the honour was no; and make\n",
      "And, sir, nor him,\n",
      "And I are so, by her, and all,\n",
      "And to your lord: for\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "To the world with a noble lord,\n",
      "And in our way and with my lord,\n",
      "To give thee, we have a very duke, my mother,\n",
      "Or it well but with them; in the rest,\n",
      "My own husband, that she's brother, that I have the blood\n",
      "I\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I have I had not so well:\n",
      "Thou, not my name of this'st is a very.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "GLOUCESTER:\n",
      "I have\n",
      "The brother?\n",
      "\n",
      "\n",
      "GLOUCESTER:\n",
      "DUKE OF YORK:\n",
      "I have thy hands.\n",
      "\n",
      "First Citizen:\n",
      "BENVOLIO:\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "G ANNE:\n",
      "That'st the lord, my lord, sir.\n",
      "\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well, I do you must not.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll have a tresed be:\n",
      "\n",
      "He is you, I make it?\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I'll, in me, and I do.\n",
      "\n",
      "BRUTUS:\n",
      "I do thee,\n",
      "\n",
      "GLOUCESTER:\n",
      "What, I am, if't?\n",
      "\n",
      "PETRUCHIO:\n",
      "We'll have not a king.\n",
      "\n",
      "\n",
      "PETRUCHIO:\n",
      "Is it is I have the world, my lord,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "AtisTis but,\n",
      "MENENIUS:\n",
      "Why, you, in this is my lord, I have a bo'd,\n",
      "But, I'll not.\n",
      "\n",
      "\n",
      "First Citizen:\n",
      "\n",
      "For thou, and, I know me; I am the time\n",
      "And I am we have be\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I'll come; for you,\n",
      "I do not to the king, nor you,\n",
      "But, and your heart is a man with his crown and have't.\n",
      "\n",
      "GLOUCESTER:\n",
      "What, which to the father?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "\n",
      "No, my lord, I do you\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Nay, I can be, my lord is my lord,\n",
      "If you have; and now.\n",
      "\n",
      "LADY OF YORK:\n",
      "\n",
      "\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "O, I, and your hands, my lord;\n",
      "I have not the man is\n",
      "When I have so, or our good\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "But that you, my lord, what now,\n",
      "Than to the king, and his honour\n",
      "'enbch me to am the daughter to you,\n",
      "And but, in his fags!\n",
      "\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "I shall not?\n",
      "\n",
      "LEONTES:\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I am her that now, sir, and I do?\n",
      "\n",
      "TRANIO:\n",
      "You am,'OF YORK:\n",
      "I'll be a rest that, my lord; I were so.\n",
      "\n",
      "\n",
      "LADY III:\n",
      "Then, if thou'st is, you will be.\n",
      "\n",
      "ANGELO\n"
     ]
    }
   ],
   "source": [
    "# Finetune CPU model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "config = GPT2ConfigCPU()\n",
    "#config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=16, num_steps=5000)\n",
    "ai.train(data, batch_size=4, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Loading GPT-2 model from provided models/cpu_shakespeare/pytorch_model.bin.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO:\u001b[0m\n",
      "What, my lord, sir, but you should to you.\n",
      "\n",
      "KING RICHARD III:\n",
      "Which, he shall you, as you'll be a man is at.\n",
      "\n",
      "PETRUCHIO:\n",
      "No; you, sir, which I'll not come;\n",
      "\n",
      "KING RICHARD III:\n",
      "And\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Titer the own good honour of him have a\n",
      "Of the brother's all his brother's\n",
      "We have the father is the life'st-morrow's death;\n",
      "I know the earth of this I make this.\n",
      "\n",
      "POLIXENES:\n",
      "That is, sir's lord, sir:\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "KATHARINA:\n",
      "Come, that, I mean, my lord, what's, that you,\n",
      "And, and that what is he is an lord,\n",
      "To am a motepoted, in the death,\n",
      "I shall not have I have\n",
      "To do all that can see\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "I have not.\n",
      "\n",
      "Clown:\n",
      "\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Not I\n",
      "To do you, which I pray you?\n",
      "\n",
      "LUCENTIO:\n",
      "I would you, sir, sir, let me,, he shall\n",
      "I may be a king'st's that he not,\n",
      "The\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "I can have I, I am\n",
      "I'll be my lord; it.\n",
      "\n",
      "BUCKINGHAM:\n",
      "No, we'll hear you.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "JULIET:\n",
      "To be the mind, as you, but your king and are;\n",
      "That is a house\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "First VINCENTIO:\n",
      "The brother is.\n",
      "\n",
      "MENENIUS:\n",
      "I am the matter?\n",
      "\n",
      "LEONTES:\n",
      "\n",
      "Why, good I'll be not not so:\n",
      "\n",
      "LADY CAPULET:\n",
      "I'll come,\n",
      "What, and I have in the king's son, but have not.\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "Ay, your lord, if you say\n",
      "The own life's the king.\n",
      "\n",
      "PETRUCHIO:\n",
      "\n",
      "Well, you are, I'll have him, I do to have.\n",
      "\n",
      "TRANIO:\n",
      "\n",
      "KING RICHARD II:\n",
      "I'll may to you?\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "\n",
      "\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "\n",
      "I will you be not to be to make him.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING RICHARD III:\n",
      "For I have the love my brother, 'tis a king.\n",
      "\n",
      "LUCIO:\n",
      "Why, sir, it have you you, for your husband\n",
      "I not are, and what's\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "The own eyes and you'll fear you to speak.\n",
      "\n",
      "DUKE OF GAUNT:\n",
      "And I'll know't?\n",
      "\n",
      "\n",
      "Provost:\n",
      "O, if you, sir, we have you do thee,\n",
      "I do me, it to you will, and not in thy\n",
      "Let him,\n",
      "==========\n",
      "\u001b[1mROMEO:\u001b[0m\n",
      "What, I may that I am the king shall be:\n",
      "And so no, sir, you?\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "'Tis it is my father's daughter?\n",
      "\n",
      "COMINIUS:\n",
      "We, that I know my lord, and my lord,\n",
      "I will in your friends?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CPU Shakespeare model\n",
    "ai = aitextgen(model=\"models/cpu_shakespeare/pytorch_model.bin\", config=\"models/cpu_shakespeare/config.json\",\n",
    "               vocab_file=\"models/cpu_shakespeare/aitextgen-vocab.json\", merges_file=\"models/cpu_shakespeare/aitextgen-merges.txt\")\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n",
      "INFO:aitextgen:Loading gpt2 model from /aitextgen.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ba15e746a4b5d9b8453f935fdceee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=40000.0), HTML(value='')), layout=Layout(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 40,000 sets of tokens from data/input.txt.\n",
      "WARNING:aitextgen:pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning:CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639293d62a7944588f40aa0a410fa9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      ".\n",
      "\n",
      "CORIOLANUS:\n",
      "This is the moon of my cousin?\n",
      "\n",
      "CORIOLANUS:\n",
      "I am a word more, let them go.\n",
      "\n",
      "KING HENRY VI:\n",
      "This is this, come to our v-morrow.\n",
      "\n",
      "CORIOLANUS:\n",
      "This is this, our mother; let them be gone, let them be gone.\n",
      "\n",
      "CORIOLANUS:\n",
      "I am a king, there's name'st;\n",
      "And they the other that the poor 'tis for the earth,\n",
      "Which, and, how the fverel is a man's life; or so were them,\n",
      "Is, how he in this noble eyes:\n",
      "Who, in him to the queen, the poor it be gone, they shall be gone,\n",
      "This is all the mind: yet, the end of my heart's name:\n",
      "As I come thy hand?\n",
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "O, my lord.\n",
      "\n",
      "But, for thee; whose you\n",
      "That I hear and the queen?\n",
      "\n",
      "CORIOLANUS:\n",
      "My command, and you shall be gone.\n",
      "\n",
      "CORIOLANUS:\n",
      "We are too, so he me; how he is this good's mind;\n",
      "For I cannot she\n",
      "\n",
      "CORIOLANUS:\n",
      "Than all me a noble pn:\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# Finetune GPU GP2 model\n",
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen\n",
    "\n",
    "# The name of the downloaded Shakespeare text for training\n",
    "file_name = \"data/input.txt\"\n",
    "\n",
    "# Train a custom BPE Tokenizer on the downloaded text\n",
    "# This will save two files: aitextgen-vocab.json and aitextgen-merges.txt,\n",
    "# which are needed to rebuild the tokenizer.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\"\n",
    "\n",
    "# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n",
    "# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\n",
    "# config = GPT2ConfigCPU()\n",
    "config = None\n",
    "\n",
    "# Instantiate aitextgen using the created tokenizer and config\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)\n",
    "\n",
    "# You can build datasets for training by creating TokenDatasets,\n",
    "# which automatically processes the dataset with the appropriate size.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)\n",
    "\n",
    "# Train the model! It will save pytorch_model.bin periodically and after completion.\n",
    "# On a 2016 MacBook Pro, this took ~25 minutes to run.\n",
    "#ai.train(data, batch_size=16, num_steps=5000)\n",
    "ai.train(data, batch_size=4, num_steps=5000)\n",
    "\n",
    "# Generate text from it!\n",
    "ai.generate(10, prompt=\"ROMEO:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
